#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script unifi√© de scraping - Blog du Mod√©rateur
Remplace tous les autres scrapers avec options flexibles
"""

import argparse
import sys
from core_scraper import BlogScraperCore
from mongodb_manager import MongoDBManager

def main():
    parser = argparse.ArgumentParser(description='Scraper unifi√© Blog du Mod√©rateur')
    parser.add_argument('--mode', choices=['mongo', 'multi'], default='mongo',
                       help='Mode de fonctionnement (default: mongo)')
    parser.add_argument('--count', type=int, default=30,
                       help='Nombre d\'articles √† r√©cup√©rer (default: 30)')
    parser.add_argument('--output', default='articles.json',
                       help='Fichier de sortie JSON (default: articles.json)')
    parser.add_argument('--url', default='https://www.blogdumoderateur.com/web/',
                       help='URL de base (default: web section)')
    
    args = parser.parse_args()
    
    print("üöÄ SCRAPER UNIFI√â - BLOG DU MOD√âRATEUR")
    print("=" * 60)
    print(f"üìä Mode: {args.mode}")
    print(f"üì∞ Articles: {args.count}")
    
    # Initialisation du scraper
    scraper = BlogScraperCore()
    
    try:
        if args.mode == 'multi':
            # Mode multi-pages avec sauvegarde JSON
            print(f"\nüì• Mode multi-pages - r√©cup√©ration depuis plusieurs sources")
            urls = [
                "https://www.blogdumoderateur.com/web/",
                "https://www.blogdumoderateur.com/digital/",
                "https://www.blogdumoderateur.com/social-media/",
                "https://www.blogdumoderateur.com/tech/",
                "https://www.blogdumoderateur.com/marketing/",
                "https://www.blogdumoderateur.com/web/page/2/",
                "https://www.blogdumoderateur.com/digital/page/2/",
            ]
            
            articles = scraper.fetch_articles_multi_pages(urls, args.count)
            
            if articles:
                scraper.save_to_json(articles, args.output)
                scraper.display_summary(articles)
            else:
                print("‚ùå Aucun article r√©cup√©r√©")
        
        elif args.mode == 'mongo':
            # Mode MongoDB - r√©cup√©ration et sauvegarde en base
            print(f"\nüì• Mode MongoDB - r√©cup√©ration et sauvegarde en base")
            
            # Connexion MongoDB
            print("üîó Connexion √† MongoDB...")
            db_manager = MongoDBManager()
            
            # R√©cup√©ration multi-pages par d√©faut pour MongoDB
            urls = [
                "https://www.blogdumoderateur.com/web/",
                "https://www.blogdumoderateur.com/digital/",
                "https://www.blogdumoderateur.com/social-media/",
                "https://www.blogdumoderateur.com/tech/",
                "https://www.blogdumoderateur.com/marketing/",
            ]
            
            articles = scraper.fetch_articles_multi_pages(urls, args.count)
            
            if articles:
                print(f"\nüíæ Sauvegarde de {len(articles)} articles en MongoDB...")
                saved_count = db_manager.save_articles(articles)
                
                print(f"\n‚úÖ TERMIN√â!")
                print(f"   ‚Ä¢ {len(articles)} articles r√©cup√©r√©s")
                print(f"   ‚Ä¢ {saved_count} articles sauvegard√©s")
                
                # Statistiques finales
                print(f"\nüìä STATISTIQUES DE LA BASE:")
                db_manager.get_stats()
            else:
                print("‚ùå Aucun article r√©cup√©r√©")
            
            db_manager.close()
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Arr√™t demand√© par l'utilisateur")
        sys.exit(0)
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()